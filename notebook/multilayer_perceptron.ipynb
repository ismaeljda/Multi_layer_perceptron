{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7b84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aac9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ml_perceptron:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, hidden_layers=(32, 32, 32), learning_rate=0.01, num_iterations=10000):\n",
    "        # Initialisation des paramètres\n",
    "        self.X = X_train.T\n",
    "        self.y = y_train.T\n",
    "        self.X_test = X_test.T\n",
    "        self.y_test = y_test.T      \n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "        if isinstance(hidden_layers, int):\n",
    "            dimensions = [hidden_layers]  # Convert single integer to list with one element\n",
    "        else:\n",
    "            dimensions = list(hidden_layers)\n",
    "            \n",
    "        dimensions.insert(0, self.X.shape[0])  \n",
    "        dimensions.append(self.y.shape[0])    \n",
    "        self.parametre = self.initialisation(dimensions)\n",
    "        self.loss_train = []\n",
    "        self.loss_test = []\n",
    "        self.acc_train = []\n",
    "        self.acc_test = []\n",
    "\n",
    "    def initialisation(self, dimensions):\n",
    "        parametre = {}\n",
    "        C = len(dimensions)\n",
    "        for c in range(1,C):\n",
    "            parametre['W' + str(c)] = np.random.randn(dimensions[c], dimensions[c - 1])\n",
    "            parametre['b' + str(c)] = np.random.randn(dimensions[c], 1)\n",
    "        return parametre\n",
    "\n",
    "    def forward_propagation(self, X=None):\n",
    "        # Allow passing X as parameter or use self.X by default pour le graph X_test\n",
    "        if X is None:\n",
    "            X = self.X\n",
    "        parametre = self.parametre\n",
    "        C = len(self.parametre) // 2\n",
    "        activations = {'A0': X}\n",
    "        for c in range(1, C + 1):\n",
    "            Z = parametre['W' + str(c)].dot(activations['A' + str(c - 1)]) + parametre['b' + str(c)]\n",
    "            Z = np.clip(Z, -500, 500)\n",
    "            activations['A' + str(c)] = 1 / (1 + np.exp(-Z))\n",
    "        return activations\n",
    "\n",
    "    def log_loss(self, A, y):\n",
    "        # Calcul de la fonction de perte logistique\n",
    "        m = y.shape[1]  \n",
    "        A = np.clip(A, 1e-15, 1 - 1e-15)  # Prevent log(0)\n",
    "        erreur = (-1 / m) * np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))\n",
    "        return erreur\n",
    "\n",
    "    def back_propagation(self, activations): \n",
    "        # Calcul des gradients (dérivées)\n",
    "        m = self.y.shape[1]\n",
    "        C = len(self.parametre) // 2 \n",
    "        parametre = self.parametre\n",
    "        dZ = activations['A' + str(C)] - self.y\n",
    "        gradients = {}\n",
    "        for c in reversed(range(1, C + 1)):\n",
    "            gradients['dW' + str(c)] = (1/m) * np.dot(dZ, activations['A' + str(c - 1)].T) \n",
    "            gradients['db' + str(c)] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if c > 1:\n",
    "                dZ = np.dot(parametre['W' + str(c)].T, dZ) * activations['A' + str(c - 1)] * (1 - activations['A' + str(c - 1)])  # Fix: use activations\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def update(self, gradient, learning_rate):\n",
    "        C = len(self.parametre) // 2\n",
    "        parametre = self.parametre\n",
    "        for c in range(1, C + 1):\n",
    "            parametre['W' + str(c)] = parametre['W' + str(c)] - learning_rate * gradient['dW' + str(c)]\n",
    "            parametre['b' + str(c)] = parametre['b' + str(c)] - learning_rate * gradient['db' + str(c)]\n",
    "\n",
    "        return parametre\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        # Allow passing X as parameter or use self.X by default\n",
    "        if X is None:\n",
    "            X = self.X\n",
    "            \n",
    "        activation = self.forward_propagation(X)\n",
    "        C = len(self.parametre) // 2 \n",
    "        return activation['A' + str(C)] >= 0.5\n",
    "\n",
    "    def cost_plot(self):\n",
    "        # Tracer l'évolution de la fonction de coût\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.loss_train, label='Train Loss')\n",
    "        plt.plot(self.loss_test, label='Test Loss')\n",
    "        plt.xlabel(\"Nombre itérations\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Evolution de la fonction de perte\")\n",
    "        plt.show()\n",
    "    \n",
    "    def acc_plot(self):\n",
    "        # Tracer l'évolution de l'accuracy'\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.acc_train, label='Train Accuracy')\n",
    "        plt.plot(self.acc_test, label='Test Accuracy')\n",
    "        plt.xlabel(\"Nombre itérations\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Evolution de l'accuracy\")\n",
    "        plt.show()\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        # Descente de gradient pour l'entraînement du modèle\n",
    "        for i in tqdm(range(self.num_iterations)):\n",
    "            if i % 100 == 0:\n",
    "                C = len(self.parametre) // 2\n",
    "                # Train metrics\n",
    "                activations_train = self.forward_propagation(self.X)\n",
    "                self.loss_train.append(self.log_loss(activations_train['A' + str(C)], self.y))\n",
    "                y_pred_train = self.predict(self.X)\n",
    "                self.acc_train.append(accuracy_score(self.y.flatten(), y_pred_train.flatten()))\n",
    "                \n",
    "                # Test metrics\n",
    "                activations_test = self.forward_propagation(self.X_test)\n",
    "                self.loss_test.append(self.log_loss(activations_test['A' + str(C)], self.y_test))\n",
    "                y_pred_test = self.predict(self.X_test)\n",
    "                self.acc_test.append(accuracy_score(self.y_test.flatten(), y_pred_test.flatten()))\n",
    "            \n",
    "            # Training step\n",
    "            activations = self.forward_propagation()\n",
    "            gradients = self.back_propagation(activations) \n",
    "            self.update(gradients, self.learning_rate)\n",
    "        \n",
    "        # Display results\n",
    "        self.cost_plot()\n",
    "        self.acc_plot()\n",
    "        y_pred = self.predict(self.X_test)\n",
    "        print(classification_report(self.y_test.flatten(), y_pred.flatten())) \n",
    "        return self.parametre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
